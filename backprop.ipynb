{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMiaWMovYj6joAl2jP/ePfM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Afitr06/backpropagation2/blob/main/backprop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zq-92EOP9P7r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow as tf\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras.layers import Input, Activation, Dense\n",
        "from keras.optimizers import SGD\n",
        "import matplotlib.pylab as plt\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import load_model\n",
        "from keras import backend as K\n",
        "from sklearn import metrics\n",
        "\n",
        "def read_file(filename):\n",
        "    file = pd.read_excel(filename)\n",
        "    file = file.values.tolist()\n",
        "    return file\n",
        "\n",
        "def load_data(filename):\n",
        "    df = pd.read_excel(filename)\n",
        "    X = np.array(df.drop(['jenis_kelamin', 'semester', 'ipk', 'target','keterangan'], axis=1))\n",
        "    Y = np.array(df['target'])\n",
        "    return X, Y\n",
        "\n",
        "\n",
        "\n",
        "def train_test(x, y, train_split=0.8):\n",
        "    TRAIN_SPLIT = train_split\n",
        "    X, Y = shuffle(x, y)\n",
        "    X_train, X_test = X[:int(TRAIN_SPLIT * len(X))], X[int(TRAIN_SPLIT * len(X)):]\n",
        "    Y_train, Y_test = Y[:int(TRAIN_SPLIT * len(Y))], Y[int(TRAIN_SPLIT * len(Y)):]\n",
        "\n",
        "    return X_train, X_test, Y_train, Y_test\n",
        "\n",
        "def train_test2(x, y, train_split=0.8):\n",
        "    TRAIN_SPLIT = train_split\n",
        "    X, Y = shuffle(x, y)\n",
        "    X_train, X_test = X[:int(TRAIN_SPLIT * len(X))], X[int(TRAIN_SPLIT * len(X)):]\n",
        "    Y_train, Y_test = Y[:int(TRAIN_SPLIT * len(Y))], Y[int(TRAIN_SPLIT * len(Y)):]\n",
        "    Y_test2 = Y_test.copy()\n",
        "    return X_train, X_test, Y_train, Y_test, Y_test2\n",
        "\n",
        "\n",
        "#convert label to categorical\n",
        "def label_cat(Y_train, Y_test):\n",
        "    Y_train = to_categorical(Y_train, num_classes=None)\n",
        "    Y_test = to_categorical(Y_test, num_classes=None)\n",
        "    return Y_train, Y_test\n",
        "\n",
        "def train(x, y, learning_rate=0.04, batch_size=7, n_epochs=1000, layerx=5,aktiv1=\"relu\"):\n",
        "    np.random.seed(7)\n",
        "    X_train, X_test, Y_train, Y_test = train_test(x, y)\n",
        "    Y_train, Y_test = label_cat(Y_train,Y_test)\n",
        "\n",
        "\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(10, input_dim=7, activation='relu'))\n",
        "    model.add(Dense(5, activation='relu'))\n",
        "    model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "    # Compile model\n",
        "    sgd = SGD(lr=learning_rate)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "    # Fit the model\n",
        "    backprop = model.fit(X_train, Y_train, epochs=n_epochs, batch_size=batch_size, verbose=2)\n",
        "\n",
        "\n",
        "    scores = model.evaluate(X_test, Y_test)\n",
        "    model.save('./media/model/model_backprop.h5')\n",
        "\n",
        "    matriks = model.metrics_names[1]\n",
        "    skor = scores[1]*100\n",
        "    K.clear_session()\n",
        "    return backprop, matriks, skor\n",
        "\n",
        "def predict(data):\n",
        "    y_dict = {\n",
        "        1 : 'Kurang',\n",
        "        2 : 'Cukup',\n",
        "        3 : 'Baik',\n",
        "        4 : 'Memuaskan',\n",
        "        5 : 'Cumlaude'\n",
        "    }\n",
        "\n",
        "\n",
        "def train2(x, y, learning_rate=0.3, batch_size=50, n_epochs=1000, layerx = [10,10,5], aktivx = ['relu', 'relu', 'softmax']):\n",
        "    np.random.seed(7)\n",
        "    X_train, X_test, Y_train, Y_test, Y_test2 = train_test2(x, y)\n",
        "    Y_train, Y_test = label_cat(Y_train,Y_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(int(layerx[0]), input_dim=7, activation=aktivx[0]))\n",
        "    for idx, val in enumerate(layerx):\n",
        "        if idx == 0:\n",
        "            temp = 0\n",
        "        else:\n",
        "            model.add(Dense(int(layerx[idx]), activation=aktivx[idx]))\n",
        "    model.add(Dense(6, activation='softmax'))\n",
        "    # model.add(Dense(int(layerx[2]), activation=aktivx[2]))\n",
        "    # model.add(Dense(5, activation='relu'))\n",
        "    # model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "    # Compile model\n",
        "    sgd = SGD(lr=learning_rate)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "    # Fit the model\n",
        "    backprop = model.fit(X_train, Y_train, epochs=n_epochs, batch_size=batch_size, verbose=2)\n",
        "    print(aktivx)\n",
        "    print(layerx)\n",
        "\n",
        "    scores = model.evaluate(X_test, Y_test)\n",
        "    model.save('./media/model/model_backprop.h5')\n",
        "\n",
        "    matriks = model.metrics_names[1]\n",
        "    skor = scores[1]*100\n",
        "    #print (metrics.classification_report(Y_test, clf.predict(X_test)))\n",
        "   # print (\"accuracy:\" ,metrics.accuracy_score(Y_test, clf.predict(X_test)))\n",
        "\n",
        "    hasil = model.predict(X_test)\n",
        "    confus_dict = {\n",
        "        1 : 1,\n",
        "        2 : 2,\n",
        "        3 : 3,\n",
        "        4 : 4,\n",
        "        5 : 5\n",
        "    }\n",
        "\n",
        "    index_max = np.argmax(hasil[21])\n",
        "    predicted = []\n",
        "    for i in range(len(hasil)):\n",
        "        index = np.argmax(hasil[i])\n",
        "        predicted.append(confus_dict[index])\n",
        "\n",
        "    print(predicted)\n",
        "    print(Y_test2)\n",
        "    confus_matrix = metrics.classification_report(Y_test2, predicted, labels=[1, 2, 3, 4, 5])\n",
        "    print(confus_matrix)\n",
        "\n",
        "\n",
        "    K.clear_session()\n",
        "    return backprop, matriks, skor\n",
        "\n",
        "def predict(data):\n",
        "    y_dict = {\n",
        "        1 : 'Kurang',\n",
        "        2 : 'Cukup',\n",
        "        3 : 'Baik',\n",
        "        4 : 'Memuaskan',\n",
        "        5 : 'Cumlaude'\n",
        "    }\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     file = 'training.xlsx'\n",
        "#     x, y = load_data(file)\n",
        "#     backprop, matriks, skor = train(x, y)\n",
        "#     print(print(\"\\n%s: %.2f%%\" % (matriks, skor)))\n",
        "    # df = pd.read_excel(file)\n",
        "    # print(df)"
      ]
    }
  ]
}